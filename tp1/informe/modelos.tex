\section{Modelos}
\subsection{Árbol de decisión}
El primer modelo que utilizamos es el más fácil para overfittear, y utilizamos distintos efoques greedy con el objetivo de reducir la dimensión del árbol.
La librería no soporta \textit{prunning}, así que nos centramos en la altura del árbol y la cantidad de muestras por nodo.
\begin{itemize}
\item \textbf{max\_depth:} \textit{3, 5, 10} - La altura del árbol
\item \textbf{criterion:} \textit{Gini, Entropy} - La métrica utilizada.
\item \textbf{max\_features:} \textit{10, 50, 100} - Cuantos atributos tomar en cuenta para la partición.
\item \textbf{max\_samples\_leaf:} \textit{1, 5 10} - Cuantas muestras puede haber en cada hoja
\end{itemize}
{\Large CUIDADO VERSO}

Los árboles tienen el problema de ser condicionados a favor de la clase mayoritaria del dataset, y, aunque en general esto no es bueno,
en nuestro caso particular de tener más emails válidos que spam no nos trae problema, porque es peor identificarlos como spam (\textit{false positive}) que errar por preacución (\textit{false negative}).

\subsection{Support Vector Machines}
La librería ofrece distintas implementaciones de \textit{SVM}, basados en diferentes formulaciones matemáticas y con muchas posiblidades de customización,
como acotar la cantidad de \textit{support vectors} o usar \textit{kernels} propios. Nos acotamos a probar con 3 tipos distintos de kernels:
\begin{itemize}
\item \textbf{kernel:} \textit{linear, polynomial, rbf} - El tipo de kernel
\end{itemize}

\subsection{Random Forest}
Este clasificador consiste en entrenar un conjunto de arboles de decision de forma aleatoria, para tratar de minimizar el sobreajuste que puede aparecer al entrenar un solo arbol.
A diferencia de lo visto en clase, la implementación de este modelo reemplaza la votación de una única clase por el promedio de las predicciones probabilísticas.
 Los parámetros con los que experimentamos son los mismos que para un único árbol de decisión, pero además de esos, se le agrega:
\begin{itemize}
\item \textbf{n\_estimators:} \textit{2, 5, 7, 10, 15} - La cantidad de árboles
\item \textbf{criterion:} \textit{"gini", "entropy"} - La metrica utilizada
\item \textbf{max\_features:} \textit{"sqrt", "None"} - Cuantos atributos a tener en cuenta para la particion.
\end{itemize}

\subsection{K Nearest Neighbors}
Este modelo consiste en clasificar una nueva instancia en base a la frecuencia de las K instancias mas cercanas.
Para experimentar con este modelo decidimos utilizar la implementación del clasificador KNeighborsClassifier de la libreria de \text{sklearn}.
A este clasificador se le deben pasar como parametro la cantidad de vecinos contra los cuales calcular la distancia y algunos otros mas que detallaremos a continuación:
\begin{itemize}
\item \textbf{n\_neighbors:} \textit{1, 3, 5, 7, 10} - La cantidad de vecinos sobre los cuales calcular la distancia
\item \textbf{weights:} \textit{uniform, distance} - La función de peso que es utilizada a la hora de predecir
\end{itemize}
A la hora de experimentar, decidimos utilizar una cantidad acotada de vecinos(es decir un bajo valor de k), ya que al ser una base de datos con 90K mails, este clasificador tarda demasiado
en terminar y en muchos casos se obtienen errores de memoria, ya que debe calcular la distancia de la nueva instancia contra todas los demas.



\subsection{Naive Bayes}

Para este algoritmo encontramos en la libreria Sklearn tres implementacions distintas. Cada una supone una distribucion distinta para la probabilidad de los atributos: Gaussiana, Multinomial y Bernoulli.\\


Como no hicimos ningun tipo de analisis sobre que distribucion subyacente tienen los datos, en principio decidimos utilizar las tres. Sin embargo nos encontramos con que Bernoulli esta diseñado para trabajar con features binarias/booleanas motivo por el cual lo descartamos.


\begin{itemize}
\item \textbf{Distribucion Multinomial:}
	\begin{itemize}
	\item \textbf{alpha:} \textit{(0 , 0.001, 0.1, 1.0)} - Parametro de suavizado. (No entendemos bien como suaviza pero decidimos ver que pasaba).
	\item \textbf{fit\_prior:} \textit{[True, False]} Sirve para aprender sobre las probabilidades a priori de las clases. Si es falso, se utilizara probabilidad uniforme.
	\end{itemize}


\item \textbf{Distribucion Gaussiana:} La distribucion gaussiana segun la documentacion tiene parametros, pero en la implentacion cuando intentamos utilizarlos nos dicen que no existen.

\end{itemize}





