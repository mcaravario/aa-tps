\section{Modelos}

\subsection{Árbol de decisión}
El primer modelo que utilizamos es el más fácil para overfittear, y utilizamos distintos efoques greedy con el objetivo de
reducir la dimensión del árbol. La librería de sklearn no soporta \textit{prunning}, así que nos centramos en la altura del árbol
y la cantidad de muestras por nodo.
\begin{itemize}
\item \textbf{max\_depth:} \textit{3, 5, 10} - La altura del árbol
\item \textbf{criterion:} \textit{Gini, Entropy} - La métrica utilizada.
\item \textbf{max\_features:} \textit{10, 50, 100} - Cuantos atributos tomar en cuenta para la partición.
\item \textbf{max\_samples\_leaf:} \textit{1, 5 10} - Cuantas muestras puede haber en cada hoja
\end{itemize}


\subsection{Support Vector Machines}
La librería ofrece distintas implementaciones de \textit{SVM}, basados en diferentes formulaciones matemáticas y con muchas posibilidades de customización, como acotar la cantidad de \textit{support vectors} o usar \textit{kernels} propios. Nos acotamos a probar con los siguientes parámetros:
\begin{itemize}
\item \textbf{kernel:} \textit{linear, polynomial, rbf} - El tipo de kernel
\item \textbf{C:} \textit{1.0, 1.5, 2, 2.5, 3, 3.5} - Parámetro de penalización para los errores (se permiten algunos errores, pero se los penaliza)
\end{itemize}

\subsection{Random Forest}
Este clasificador consiste en entrenar un conjunto de arboles de decision de forma aleatoria, para tratar de minimizar el sobreajuste que puede aparecer al entrenar un solo arbol.
A diferencia de lo visto en clase, la implementación de este modelo reemplaza la votación de una única clase por el promedio de las predicciones probabilísticas.
 Los parámetros con los que experimentamos son los mismos que para un único árbol de decisión, pero además de esos, se le agrega:
\begin{itemize}
\item \textbf{n\_estimators:} \textit{2, 5, 7, 10, 15} - La cantidad de árboles
\item \textbf{criterion:} \textit{"gini", "entropy"} - La metrica utilizada
\item \textbf{max\_features:} \textit{"sqrt", "None"} - Cuantos atributos a tener en cuenta para la particion.
\end{itemize}


\subsection{K Nearest Neighbors}
Este modelo consiste en clasificar una nueva instancia en base a la frecuencia de las K instancias mas cercanas.
Para experimentar con este modelo decidimos utilizar la implementación del clasificador KNeighborsClassifier de la libreria de \text{sklearn}.
A este clasificador se le deben pasar como parametro la cantidad de vecinos contra los cuales calcular la distancia y algunos otros mas que detallaremos a continuación:
\begin{itemize}
\item \textbf{n\_neighbors:} \textit{1, 3, 5, 7, 10} - La cantidad de vecinos sobre los cuales calcular la distancia
\item \textbf{weights:} \textit{uniform, distance} - La función de peso que es utilizada a la hora de predecir
\end{itemize}
A la hora de experimentar, decidimos utilizar una cantidad acotada de vecinos(es decir un bajo valor de k), ya que al ser una base de datos con 90K mails, este clasificador tarda demasiado
en terminar y en muchos casos se obtienen errores de memoria, ya que debe calcular la distancia de la nueva instancia contra todas los demas.


\subsection{Naive Bayes}
Para este algoritmo encontramos en la librería sklearn tres implementaciones distintas. Cada una supone una distribución
distinta para la probabilidad de los atributos: Gaussiana, Multinomial y Bernoulli.\\
Como no hicimos ningún tipo de análisis sobre que distribución subyacente tienen los datos, en principio decidimos utilizar
las tres. Sin embargo nos encontramos con que Bernoulli esta diseñado para trabajar con features binarias/booleanas motivo
por el cual lo descartamos.

\begin{itemize}
\item \textbf{Distribución Multinomial:}
        \begin{itemize}
        \item \textbf{alpha:} \textit{(0 , 0.001, 0.1, 1.0)} - Parámetro de suavizado.
        \item \textbf{fit\_prior:} \textit{[True, False]} Se utiliza para aprender sobre las probabilidades a priori de las clases. Si es falso, se utilizara probabilidad uniforme.
        \end{itemize}
\item \textbf{Distribución Gaussiana:} La distribución gausiana según la documentación tiene parámetros, pero en la implementación cuando intentamos utilizarlos nos dicen que no existen.
\end{itemize}
