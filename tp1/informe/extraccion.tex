\section{Extracción de atributos}
\subsection{Bag of Words}
Como los emails que se busca clasificar están en formato texto plano, es necesario transformar la colección de datos en los vectores de atributos numéricos que recibirán los modelos. A este proceso se le llama \textit{vectorización}.

Decidimos utilizar la estrategia de representación \textbf{\textit{Bag of Words}}, en la cual un atributo es la frequencia de ocurrencia de una palabra. La idea general es presuponer que existen ciertas palabras que aparecen más (o menos) veces en los email spam que en los ham. 

Existen diversos grados de refinamiento de esta estrategia, que derivan del área de \textit{Procesamiento de Lenguaje Natural}. Nosotros utilizamos la función \textbf{TfidVectorizer}, que une las funciones \textbf{Count Vectorizer} y \textbf{Inverse Document Frequency}. Estas sirven para \textit{tokenizar}, \textit{contar} y \textit{normalizar} las palabras de los textos.

La primera función asigna tokens a las palabras y cuenta las ocurrencias, pero el problema es que las palabras que se repiten mucho en cualquier lenguaje, como los pronombres, artículos, etc., no aportan mucha información al clasificador. Por esto, la segunda función aporta la técnica de \textbf{términos pesados}, en la cual se normaliza los textos para restarle importancia a esas palabras que más se repiten. Esta idea se aplica a otros ámbitos académicos, como los resultados de motores de búsqueda.

Además de lo explicado anteriormente, seleccionamos estos otros parámetros:
\begin{itemize}
\item \textbf{max\_features:} \textit{100} - Para acotar la cantidad de atributos, por default utiliza todos los tokens posibles.
\item \textbf{stop\_words:} \textit{english} - Para que elimine las palabras más comunes del idioma, como ``a'', ``it'', ``its'', etc.\\

No se si el siguente está bien conceptualmente, si quieren saquenlo.
\item \textbf{lowercase:} \textit{False} - No transforme las palabras a minúscula por default. Algunos spam están completamente escritos en mayúscula.
\end{itemize}

{\Large WARNING!!!! POSIBLE VERSO
CYNTIA: ES LO QUE HABLAMOS CON FACUNDO LE AGREGUE DOS LINEAS AL FINAL SI LES PARECE LO DEJAMOS ASI Y BORRAMOS EL COMENTARIO}

Decidimos no utilizar \textit{Grid Search} sobre estos parámetros, en particular max\_features, porque creemos que se podría caer en \textit{overfitting} fácilmente. Al utilizar la misma cantidad de atributos para todos los modelos, comparten la representación abstracta del conjunto de emails. Por otro lado esto reduce mucho nuestro espacio de búsqueda haciendo que correr los algoritmos sea una tarea posible en un tiempo razonable.\\

{\Large SI QUIEREN PODEMOS CODEAR LA SOLUCION CON LEMMATIZER, NO ES NADA, PERO NO TENGO GANAS DE HACERLO AHORA
CYNTIA: YO CREO QUE ESTA BIEN ASI, BORRARIA ESTE COMENTARIO Y LISTO}
\subsection{Limitaciones}

Al decidir usar solo la técnica de \textit{Bag of Words} podríamos estar perdiendo atributos importantes que determinan si un mail es o no spam. Podríamos haber hecho un análisis sobre la estructura de los mismos y buscar (parseando los textos) atributos que nos parecieran representativos del problema como: largo del texto, características de los encabezamos, etc. Sin embargo como obtuvimos buenos resultados utilizando solo la estrategia \textit{Bag of Words} nos pareció que no era necesario.   \\

La limitación inherente a \textit{Bag of Words} es que simplifica la información que se podría obtener de los textos, ignorando cualquier dependencia en el orden de las palabras y tomando como tokens distintos errores ortográficos o derivaciones de palabras. Esto se puede resolver modificando más parámetros (\textit{n-grams}) o con distintas técnicas de \textit{NLP}, como por ejemplo, \textbf{lemmatizers}, pero decidimos no utilizarlas para no aumentar la complejidad.
