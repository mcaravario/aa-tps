\newcommand\MyBox[2]{
  \fbox{\lower0.75cm
    \vbox to 1.7cm{\vfil
      \hbox to 1.7cm{\hfil\parbox{1.4cm}{#1\\#2}\hfil}
      \vfil}%
  }%
}

\noindent
\renewcommand\arraystretch{1.5}
\setlength\tabcolsep{0pt}

\section{Resultados}
A la hora de experimentar, decidimos utilizar el método \textbf{grid search} combinado con \textbf{cross validation}, provisto por la librería \textit{sklearn} de python.
Este método se encarga de armar un clasficador por cada combinación de parámetros que se le pasan y luego aplica la estrategia de \textbf{KFold} (con k = 10), la cual
consiste en particionar nuestro conjunto de datos de test en 10 subconjuntos, y entrenar sobre 9 de ellos, dejando un sobconjunto para testear.

Otra decisión que tomamos al experimentar, fue la de tomar la mitad del set de datos(45K mails) como datos de desarrollo y la otra mitad como datos
de test. Esta decisión fue tomada con el objetivo de obtener una estimación mas realista sobre la performance utilizada y ademas en muchos de los modelos
utilizados, entrenar sobre una cantidad de datos tan grande puede llegar a tardar demasiado.

Los resultados obtenidos se encuentran presentados a continuación

\subsection{Decision Tree}
En el caso de Decision Tree la mejor configuración de parametros que obtuvimos con el grid search fue la siguiente:
\begin{itemize}
\item{fit\_prior: True}
\item{alpha: 0}
\end{itemize}


Con un score de 0.977214284117

 \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
   \multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft actual\\ value}} &
   & \multicolumn{2}{c}{\bfseries Random Forest} & \\
   & & \bfseries p & \bfseries n & \bfseries total \\
   & p$'$ & \MyBox{22130}{} & \MyBox{370}{} & P$'$ \\[2.4em]
   & n$'$ & \MyBox{649}{} & \MyBox{21851}{} & N$'$ \\
   & total & P & N &
 \end{tabular}

\subsection{SVM}
\subsection{Random Forest}
En el caso de random forest la mejor configuración de parametros que obtuvimos con el grid search fue la siguiente:
\begin{itemize}
\item{max\_features: "sqrt"}
\item{n\_estimators: 15}
\item{criterion: "entropy"}
\end{itemize}

Con un score de 0.980175963637

 \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
   \multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft actual\\ value}} &
   & \multicolumn{2}{c}{\bfseries Random Forest} & \\
   & & \bfseries p & \bfseries n & \bfseries total \\
   & p$'$ & \MyBox{22339}{} & \MyBox{161}{} & P$'$ \\[2.4em]
   & n$'$ & \MyBox{720}{} & \MyBox{21780}{} & N$'$ \\
   & total & P & N &
 \end{tabular}

\section{K Nearest Neighbors}

En el caso de K nearest neighbors, la mejor configuracion de parametros fue la siguiente:
\begin{itemize}
  \item{n\_neighbors: 1}
  \item{weights: "uniform"}
\end{itemize}

\textbf{------------------- MODIFICAR ESTO -------------------------} \\
obteniendo un score de: 0.908566

\begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
  \multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft actual\\ value}} &
  & \multicolumn{2}{c}{\bfseries K Nearest Neighbors} & \\
  & & \bfseries p & \bfseries n & \bfseries total \\
  & p$'$ & \MyBox{21800}{} & \MyBox{700}{} & P$'$ \\[2.4em]
  & n$'$ & \MyBox{8855}{} & \MyBox{13645}{} & N$'$ \\
  & total & P & N &
\end{tabular}

\textbf{------------------- MODIFICAR ESTO -------------------------}

Una conclusión importante que obtuvimos al experimentar con diversos \textit{k}, fue que a medidad que este aumentaba,
el score obtenido disminuia de manera proporcional. Es por esto que el grid search eligio un \textit{k} muy bajo como el mejor.

Otra particularidad que notamos, fue que al usar el algoritmo de \textit{distance} para calcular la distancia a los vecinos mas cercanos,
los resultados fueron mejores que al hacerlo con el método \textit{uniform}.

\subsection{Naive Bayes - Gaussian}

En el caso de Naive Bayes - Gaussian. En este caso, como mencionamos mas arriba, no tenemos ningun parametro por variar. \\

Con un score de 0.740670375899

 \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
   \multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft actual\\ value}} &
   & \multicolumn{2}{c}{\bfseries Random Forest} & \\
   & & \bfseries p & \bfseries n & \bfseries total \\
   & p$'$ & \MyBox{21800}{} & \MyBox{700}{} & P$'$ \\[2.4em]
   & n$'$ & \MyBox{8855}{} & \MyBox{13645}{} & N$'$ \\
   & total & P & N &
 \end{tabular}


\subsection{Naive Bayes - Multinomial}

En el caso de Naive Bayes - Multinomial la mejor configuración de parametros que obtuvimos con el grid search fue la siguiente:
\begin{itemize}
\item{fit\_prior: True}
\item{alpha: 0}
\end{itemize}

Con un score de 0.815540223063

 \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
   \multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft actual\\ value}} &
   & \multicolumn{2}{c}{\bfseries Random Forest} & \\
   & & \bfseries p & \bfseries n & \bfseries total \\
   & p$'$ & \MyBox{21052}{} & \MyBox{1448}{} & P$'$ \\[2.4em]
   & n$'$ & \MyBox{6011}{} & \MyBox{16489}{} & N$'$ \\
   & total & P & N &
 \end{tabular}

