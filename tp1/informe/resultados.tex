\newcommand\MyBox[2]{
  \fbox{\lower0.75cm
    \vbox to 1.7cm{\vfil
      \hbox to 1.7cm{\hfil\parbox{1.4cm}{#1\\#2}\hfil}
      \vfil}%
  }%
}

\noindent
\renewcommand\arraystretch{1.5}
\setlength\tabcolsep{0pt}

\section{Resultados}
A la hora de experimentar, decidimos utilizar el método \textbf{grid search} combinado con \textbf{cross validation}, provisto por la librería \textit{sklearn} de python.
Este método se encarga de armar un clasificador por cada combinación de parámetros que se le pasan y luego aplica la estrategia de \textbf{KFold} (con k = 10), la cual
consiste en particionar nuestro conjunto de datos de test en 10 subconjuntos, y entrenar sobre 9 de ellos, dejando un subconjunto para testear.\\

Otra decisión que tomamos al experimentar, fue la de tomar la mitad del set de datos(45K mails) como datos de desarrollo y la otra mitad como datos
de test. Esta decisión fue tomada con el objetivo de obtener una estimación mas realista sobre la performance utilizada y ademas en muchos de los modelos
utilizados, entrenar sobre una cantidad de datos tan grande puede llegar a tardar demasiado.

Los resultados obtenidos se encuentran presentados a continuación

\subsection{Decision Tree}
En el caso de Decision Tree la mejor configuración de parametros que obtuvimos con el grid search fue la siguiente:
\begin{itemize}
\item{fit\_prior: True}
\item{alpha: 0}
\end{itemize}


Con un score de 0.977214284117

 \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
   \multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft actual\\ value}} &
   & \multicolumn{2}{c}{\bfseries Random Forest} & \\
   & & \bfseries p & \bfseries n & \bfseries total \\
   & p$'$ & \MyBox{22130}{} & \MyBox{370}{} & P$'$ \\[2.4em]
   & n$'$ & \MyBox{649}{} & \MyBox{21851}{} & N$'$ \\
   & total & P & N &
 \end{tabular}

\subsection{SVM}

En el caso de random forest la mejor configuración de parametros que obtuvimos con el grid search fue la siguiente:
\begin{itemize}
\item{kernel: l}
\item{C: 3.5}
\end{itemize}

Con un score de 0.94790000457

 \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
   \multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft actual\\ value}} &
   & \multicolumn{2}{c}{\bfseries Random Forest} & \\
   & & \bfseries p & \bfseries n & \bfseries total \\
   & p$'$ & \MyBox{21979}{} & \MyBox{521}{} & P$'$ \\[2.4em]
   & n$'$ & \MyBox{1759}{} & \MyBox{20741}{} & N$'$ \\
   & total & P & N &
 \end{tabular}



\subsection{Random Forest}
En el caso de random forest la mejor configuración de parametros que obtuvimos con el grid search fue la siguiente:
\begin{itemize}
\item{max\_features: "sqrt"}
\item{n\_estimators: 15}
\item{criterion: "entropy"}
\end{itemize}

Con un score de 0.980175963637

 \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
   \multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft actual\\ value}} &
   & \multicolumn{2}{c}{\bfseries Random Forest} & \\
   & & \bfseries p & \bfseries n & \bfseries total \\
   & p$'$ & \MyBox{22339}{} & \MyBox{161}{} & P$'$ \\[2.4em]
   & n$'$ & \MyBox{720}{} & \MyBox{21780}{} & N$'$ \\
   & total & P & N &
 \end{tabular}

\section{K Nearest Neighbors}
Para este caso cabe mencionar que los resultados que presentaremos a continuación fueron obtenidos
a partir de la experimentación sin usar el método de reducción de dimensionalidad. Esto se debe a que
cuando experimentamos con la reduccion de los KBest, nuestro modelo entrenaba de manera correcta seleccionando
 una mejor configuracion de parametros, pero a la hora de predecir resultados sobre nuestros datos de test,
obteniamos un error de memoria. Para tratar de solucionar esto decidimos bajar la cantidad de features de 100 a 50
y finalmente a 20, obteniendo en todos los casos el mismo error. Es por esto que decidimos no utilizar la reducción
de atributos obteniendo la siguiente configuracion de parametros como la mejor:

\begin{itemize}
  \item{n\_neighbors: 5}
  \item{weights: "distance"}
\end{itemize}

Con estos parametros obtuvimos un score de: 0.9866

\begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
  \multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft actual\\ value}} &
  & \multicolumn{2}{c}{\bfseries K Nearest Neighbors} & \\
  & & \bfseries p & \bfseries n & \bfseries total \\
  & p$'$ & \MyBox{22171}{} & \MyBox{329}{} & P$'$ \\[2.4em]
  & n$'$ & \MyBox{272}{} & \MyBox{22228}{} & N$'$ \\
  & total & P & N &
\end{tabular}

Una conclusión importante que obtuvimos al experimentar con diversos \textit{k}, fue que a medidad que este aumentaba,
el score obtenido disminuia de manera proporcional. Es por esto que el grid search eligio un \textit{k} muy bajo como el mejor.

Otra particularidad que notamos, fue que al usar el algoritmo de \textit{distance} para calcular la distancia a los vecinos mas cercanos,
los resultados fueron mejores que al hacerlo con el método \textit{uniform}.

\subsection{Naive Bayes - Gaussian}

En este caso, como mencionamos mas arriba, no tenemos ningun parametro por variar. \\

Con un score de 0.740670375899

 \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
   \multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft actual\\ value}} &
   & \multicolumn{2}{c}{\bfseries Random Forest} & \\
   & & \bfseries p & \bfseries n & \bfseries total \\
   & p$'$ & \MyBox{21800}{} & \MyBox{700}{} & P$'$ \\[2.4em]
   & n$'$ & \MyBox{8855}{} & \MyBox{13645}{} & N$'$ \\
   & total & P & N &
 \end{tabular}


\subsection{Naive Bayes - Multinomial}

En el caso de Naive Bayes - Multinomial la mejor configuración de parametros que obtuvimos con el grid search fue la siguiente:
\begin{itemize}
\item{fit\_prior: True}
\item{alpha: 0}
\end{itemize}

Con un score de 0.815540223063

 \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
   \multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft actual\\ value}} &
   & \multicolumn{2}{c}{\bfseries Random Forest} & \\
   & & \bfseries p & \bfseries n & \bfseries total \\
   & p$'$ & \MyBox{21052}{} & \MyBox{1448}{} & P$'$ \\[2.4em]
   & n$'$ & \MyBox{6011}{} & \MyBox{16489}{} & N$'$ \\
   & total & P & N &
 \end{tabular}
