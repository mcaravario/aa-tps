\newcommand\MyBox[2]{
  \fbox{\lower0.75cm
    \vbox to 1.7cm{\vfil
      \hbox to 1.7cm{\hfil\parbox{1.4cm}{#1\\#2}\hfil}
      \vfil}%
  }%
}

\noindent
\renewcommand\arraystretch{1.5}
\setlength\tabcolsep{0pt}

\section{Resultados}
A la hora de experimentar, decidimos utilizar el método \textbf{grid search} combinado con \textbf{cross validation}, provisto por la librería \textit{sklearn} de python.
Este método se encarga de armar un clasficador por cada combinación de parámetros que se le pasan y luego aplica la estrategia de \textbf{KFold} (con k = 10), la cual
consiste en particionar nuestro conjunto de datos de test en 10 subconjuntos, y entrenar sobre 9 de ellos, dejando un sobconjunto para testear.

Los resultados obtenidos se encuentran presentados a continuación

\subsection{Decision Tree}
\subsection{SVM}
\subsection{Random Forest}
En el caso de random forest la mejor configuración de parametros que obtuvimos con el grid search fue la siguiente:
\begin{itemize}
\item{max\_features: "sqrt"}
\item{n\_estimators: 15}
\item{criterion: "entropy"}
\end{itemize}

Con un score de 0.980175963637

 \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
   \multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft actual\\ value}} &
   & \multicolumn{2}{c}{\bfseries Random Forest} & \\
   & & \bfseries p & \bfseries n & \bfseries total \\
   & p$'$ & \MyBox{22339}{} & \MyBox{161}{} & P$'$ \\[2.4em]
   & n$'$ & \MyBox{720}{} & \MyBox{21780}{} & N$'$ \\
   & total & P & N &
 \end{tabular}

\section{K Nearest Neighbors}
